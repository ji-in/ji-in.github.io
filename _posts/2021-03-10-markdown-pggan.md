---
title: "[PGGAN] Progressive Growing of GANs for Improved Quality, Stability, and Variation"
layout: post
category: blog
use_math: true
author: jiin
---

Paper: [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)

<br>

## 1. Introduction

ê°€ì¥ ìœ ëª…í•œ generative methodsëŠ” Autoregressive models, VAEs, GANsê°€ ìˆë‹¤.

Autoregressive modelsëŠ” sharp imageë¥¼ ìƒì„±í•˜ì§€ë§Œ evaluationì´ ëŠë¦¬ê³  latent representationì„ ê°€ì§€ì§€ ì•ŠëŠ”ë‹¤.

VAEsëŠ” trainì€ ì‰½ì§€ë§Œ blurryí•œ ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

GANsë„ sharp imageë¥¼ ìƒì„±í•˜ì§€ë§Œ, small resolution(ì €í•´ìƒë„)ì—ì„œ ë™ì‘í•˜ê³  variationì´ ì œí•œì ì´ë©° trainingì´ ë¶ˆì•ˆì •í•˜ë‹¤. 

<br>

High resolution(ê³ í•´ìƒë„) ì´ë¯¸ì§€ì—ì„œ generated imageì™€ training imageëŠ” ë„ˆë¬´ ì‰½ê²Œ êµ¬ë³„ì´ ê°„ë‹¤. í™”ì§ˆì´ ë†’ê¸° ë•Œë¬¸ì— ì•„ì£¼ ì‘ì€ ì°¨ì´ë„ ì¡ì•„ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë˜ì„œ high resolutionì—ì„œì˜ ì´ë¯¸ì§€ ìƒì„±ì€ ì–´ë µë‹¤. 

ë˜í•œ large resolutionì€ ë©”ëª¨ë¦¬ ì œí•œë•Œë¬¸ì— ë” ì‘ì€ minibatchë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤.

Key insightëŠ” generatorì™€ discriminatorë¥¼ ì ì§„ì ìœ¼ë¡œ í‚¤ìš°ëŠ” ê²ƒì´ë‹¤. low-resolution ì´ë¯¸ì§€ë¶€í„° ì‹œì‘í•˜ê³  í•™ìŠµì´ ì§„í–‰ë  ë•Œ higher-resolution detailsë¥¼ ì‚¬ìš©í•˜ëŠ” ìƒˆë¡œìš´ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•œë‹¤.

<p align="center">
    <img src="..\assets\pggan\output2.gif" alt="output" style="zoom:80%;" />
    Output
</p>



<br>

## 2. Progressive Growing of GANs

<p align="center">
    <img src="..\assets\pggan\structure.PNG" alt="output" style="zoom:80%;" />
    Figure 1
</p>

Trainingì€ ì €í•´ìƒë„ 4 x 4 ì´ë¯¸ì§€ì™€ í•¨ê»˜ generator(G)ì™€ discriminator(D)ì—ì„œ ì‹œì‘í•œë‹¤. í•™ìŠµì´ ì§„í–‰ë˜ë©´, ì ì§„ì ìœ¼ë¡œ Gì™€ Dì— ë ˆì´ì–´ë¥¼ ì¶”ê°€í•´ì„œ ìƒì„± ì´ë¯¸ì§€ì˜ spatial resolutionì„ ì¦ê°€ì‹œí‚¨ë‹¤. 

ì´ ë°©ë²•ì€ ì•ˆì •ì ìœ¼ë¡œ high resolution ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ê³ , í•™ìŠµ ì†ë„ë¥¼ ìƒë‹¹íˆ ë¹ ë¥´ê²Œ í•œë‹¤.

<br>

í•™ìŠµì„ í•  ë•Œ ë„¤íŠ¸ì›Œí¬ë¥¼ ì ì§„ì ìœ¼ë¡œ í‚¤ìš°ë©´, ì´ë¯¸ì§€ì—ì„œ ì „ì²´ì ì¸ êµ¬ì¡°ë¶€í„° ì°¾ê³  ì ì°¨ ë” ì„¸ë°€í•œ ê³³ìœ¼ë¡œ ì§‘ì¤‘í•˜ê²Œ ëœë‹¤.

<p align="center">
    <img src="..\assets\pggan\output1.gif" style="zoom:80%;" />
    visual representation
</p>



Generatorì™€ discriminatorì˜ êµ¬ì¡°ëŠ” ì„œë¡œ ëŒ€ì¹­ì ì´ê³  í•­ìƒ ë™ì‹œì— ì»¤ì§„ë‹¤. í•™ìŠµì„ í•˜ëŠ” ë™ì•ˆ, ë‘ ë„¤íŠ¸ì›Œí¬ì˜ ëª¨ë“  ë ˆì´ì–´ë“¤ì€ ê³„ì† trainableí•˜ë‹¤. ë„¤íŠ¸ì›Œí¬ì— ìƒˆë¡œìš´ ë ˆì´ì–´ê°€ ì¶”ê°€ë˜ë©´, `fade in`ì„ ì‚¬ìš©í•œë‹¤.

<p align="center">
    <img src="..\assets\pggan\fade_in.PNG" alt="output" />
    Figure 2
</p>

(a)ì—ì„œ (c)ë¡œ ê°€ê¸° ìœ„í•´ (b) ê³¼ì •ì„ ê±°ì¹œë‹¤.

`2x`ëŠ” nearest neighbor filteringì„ ì‚¬ìš©í•´ì„œ image resolutionì„ ë‘ ë°°ë¡œ ë§Œë“¤ê³ , `0.5x`ëŠ” average poolingì„ ì‚¬ìš©í•´ì„œ image resolutionì„ ë°˜ìœ¼ë¡œ ì¤„ì¸ë‹¤. `toRGB`ëŠ” feature vectorsë¥¼ RGB colorsë¡œ ë°”ê¾¸ëŠ” ê³¼ì •ì´ê³ , `fromRGB`ëŠ” ê·¸ ë°˜ëŒ€ì˜ ê³¼ì •ì´ë‹¤.

16 x 16 í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ë§Œë“  í›„, ë°”ë¡œ 32 x 32 í•´ìƒë„ë¥¼ í•™ìŠµì‹œí‚¤ë©´ ì•„ì§ í•™ìŠµì´ ì•ˆëœ 32 x 32 ë ˆì´ì–´ê°€ ì˜ í•™ìŠµëœ 16 x 16 ë ˆì´ì–´ì— ì•ˆì¢‹ì€ ì˜í–¥ì„ ë¼ì¹  ìˆ˜ ìˆë‹¤. 

ê·¸ë˜ì„œ "fade in smoothly"ê°€ í•„ìš”í•˜ë‹¤.

Gì—ì„œì˜ ë™ì‘ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 

(b)ì—ì„œ 16 x 16 í•´ìƒë„ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ 2ë°°ë¡œ ëŠ˜ë ¤ í¬ê¸°ë§Œ 32 x 32ì¸ ì´ë¯¸ì§€ë¥¼ ë§Œë“ ë‹¤. 

32 x 32 ë ˆì´ì–´ì˜ ê²°ê³¼ ì´ë¯¸ì§€ì¸ ê³ í•´ìƒë„ ì´ë¯¸ì§€ì˜ ê° í”½ì…€ ê°’ì— 0~1 ì‚¬ì´ì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚´ëŠ” $\alpha$ë¥¼ ê³±í•˜ê³ , ì € í•´ìƒë„ í”½ì…€ì—ëŠ” ê·¸ ë‚˜ë¨¸ì§€ ë¹„ìœ¨ì¸ $1-\alpha$ë¥¼ ê³±í•œë‹¤. 

ë‘ ê°œì˜ ê²°ê³¼ë¥¼ ì„œë¡œ ë”í•˜ë©´ ì €í•´ìƒë„ì˜ í° ê·¸ë¦¼ê³¼ ì•ìœ¼ë¡œ í•™ìŠµì‹œí‚¬ ê³ í•´ìƒë„ì˜ ë””í…Œì¼ì´ í•©ì³ì§„ë‹¤. 

Dì—ì„œë„ ìœ„ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•œë‹¤. 

"fade in"ì„ ì‚¬ìš©í•˜ë©´, ê¸°ì¡´ì˜ ì €í•´ìƒë„ ë ˆì´ì–´ì—ì„œ í•™ìŠµí•œ ê²ƒì„ í•´ì¹˜ì§€ ì•Šê³ , ìƒˆë¡œ ì¶”ê°€ëœ ë ˆì´ì–´ë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤. 

<br>

<p align="center">
    <img src="..\assets\pggan\generator.png" />
    Generator
</p>

<p align="center">
    <img src="..\assets\pggan\discriminator.png" />
    Discriminator
</p>
<br>

## 3. Increasing Variation using Minibatch Standard Deviation

GANì€ training ì¤‘ train dataì—ì„œ ì°¾ì€ feature informationë³´ë‹¤ variationì´ ì ì€ imageë¥¼ ìƒì„±í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤. ê·¸ë¡œ ì¸í•´ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê¸° ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. Salimans et al. (2016)ì€ "minibatch discrimination"ì„ í•´ê²°ì±…ìœ¼ë¡œ ì œì‹œí–ˆë‹¤. 

ìš°ë¦¬ëŠ” "minibatch discrimination"ì„ ë‹¨ìˆœí™”í•œ minibatch standard deviationì„ ì‚¬ìš©í•´ì„œ variationì„ ì¦ê°€ì‹œí‚¨ë‹¤

Minibatch standard deviationì€ feature statisticsë¥¼ ì´ë¯¸ì§€ í•œì¥ì— ëŒ€í•´ì„œ ê³„ì‚°í•˜ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼, minibatch ì „ì²´ì—ì„œ ê³„ì‚°í•œë‹¤. ê·¸ë˜ì„œ generated imagesì˜ minibatchì™€ training imagesì˜ minibatchì—ì„œ statisticsê°€ ë¹„ìŠ·í•´ì§€ë„ë¡ í•œë‹¤.

ì´ê²ƒì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ discriminatorì˜ ëì— minibatch layerë¥¼ ì¶”ê°€í•œë‹¤. 

Minibatchì˜ ê° exampleì— ëŒ€í•´ statisticsê°€ ë§Œë“¤ì–´ì§€ê³ , ê·¸ê²ƒì„ ë ˆì´ì–´ì˜ ì¶œë ¥ì— ì—°ê²°í•˜ë©´, discriminatorê°€ statisticsë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

<br>

ê³„ì‚° ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 

* Minibatch ë‚´ì˜ ê° spatial locationì—ì„œ ê° featureì˜ í‘œì¤€í¸ì°¨ë¥¼ ê³„ì‚°í•œë‹¤. 

* ëª¨ë“  featuresì™€ spatial locationsì—ì„œ ê³„ì‚°í•œ ê°’(í‘œì¤€í¸ì°¨)ì„ í‰ê·  ë‚¸ë‹¤. 

* ê°’ì„ ë³µì‚¬í•´ì„œ ëª¨ë“  spatial locationsê³¼ minibatchì— ì—°ê²°í•´ì„œ, í•œ ê°œì˜ ì¶”ê°€ì ì¸ feature mapì„ ìƒì„±í•œë‹¤. 

ì´ ë ˆì´ì–´ëŠ” discriminatorì˜ ì–´ë””ë“ ì§€ ì‚½ì…í•  ìˆ˜ ìˆì§€ë§Œ, ëì— ì‚½ì…í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¢‹ë‹¤. 

<br>

## 4. Normalization in Generator and Discriminator

### 4.1. Equalized Learning Rate

Gaussian distributionì„ ì‚¬ìš©í•´ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™” í•˜ê³ , runtime ë™ì•ˆì— ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•œë‹¤.

$\hat{w}_i=w_i/c$ë¥¼ ì‚¬ìš©í•œë‹¤. $w_i$ëŠ” ê°€ì¤‘ì¹˜ì´ê³ , $c$ëŠ” He ì´ˆê¸°í™”ì˜ per-layer ì •ê·œí™” ìƒìˆ˜ì´ë‹¤.

ë‹¤ë¥¸ ë°©ë²•ë“¤ì€ ë³´í†µ íŒŒë¼ë¯¸í„°ì˜ scaleê³¼ ë¬´ê´€í•˜ê²Œ gradientë¥¼ ê°±ì‹ í•œë‹¤. ì´ë•Œ íŒŒë¼ë¯¸í„°ë§ˆë‹¤ dynamic rangeê°€ ë‹¤ë¥´ë©´ ì´ ê°’ì„ ì¡°ì ˆí•˜ëŠ” ë°ì— ì‹œê°„ì´ ë§ì´ ê±¸ë¦°ë‹¤.

ê·¸ëŸ¬ë‚˜ equalized learning rateë¥¼ ì‚¬ìš©í•˜ë©´, ëª¨ë“  ê°€ì¤‘ì¹˜ë“¤ì´ ë™ì¼í•œ dynamic rangeë¥¼ ê°€ì§ìœ¼ë¡œì¨ ë™ì¼í•œ learning speedë¥¼ ê°€ì§€ëŠ” ê²ƒì„ ë³´ì¥í•œë‹¤.

<br>

### 4.2. Pixelwise Feature Vector Normalization in Generator

ê° í•©ì„±ê³± ì—°ì‚° í›„ì— generatorì—ì„œ ê° í”½ì…€ì˜ feature vectorë¥¼ ë‹¨ìœ„ ê¸¸ì´ë¡œ ì •ê·œí™”í•œë‹¤. 

"local response normalization"ì˜ ë³€í˜•ì„ ì‚¬ìš©í•˜ì—¬ ì´ê²ƒì„ ìˆ˜í–‰í•œë‹¤.

<p align="center">
    <img src="..\assets\pggan\norm.PNG" style="zoom:80%;" />
</p>

$N$: feature mapì˜ ìˆ˜

$a_x,y$: pixel $(x, y)$ì—ì„œì˜ original feature vector

$b_x,y$: pixel $(x, y)$ì—ì„œì˜ normalized feature vector

ì´ ë°©ë²•ì€ ê²°ê³¼ë¥¼ ë°”ê¾¸ì§€ëŠ” ì•Šì§€ë§Œ, training ì¤‘ì— signalì˜ í¬ê¸°ê°€ ê°‘ìê¸° ì»¤ì§€ëŠ” í˜„ìƒì„ ë§‰ì•„ì¤€ë‹¤.

<br>

## 5. Multi-Scale Statistical Similarity for Assessing GAN Results

GANì˜ ê²°ê³¼ë“¤ì„ ë¹„êµí•˜ê¸° ìœ„í•´, ìˆ˜ë§ì€ ì´ë¯¸ì§€ë“¤ì„ ì‚¬ëŒì´ ì¼ì¼ì´ ë¹„êµí•´ì•¼ í•œë‹¤. ì´ ì‘ì—…ì€ í˜ë“¤ê¸° ë•Œë¬¸ì— ìë™í™”ëœ ë°©ë²•ì„ ì“´ë‹¤. 

MS-SSIMê³¼ ê°™ì€ ê¸°ì¡´ ë°©ë²•ì€ ëŒ€ëŸ‰ì˜ mode collapsesë¥¼ ì˜ ì°¾ì§€ë§Œ ìƒ‰ìƒ ë³€í™”ë‚˜ ì§ˆê° ë³€í™”ê°™ì€ ì‘ì€ íš¨ê³¼ì—ëŠ” ë°˜ì‘í•˜ì§€ ì•ŠëŠ”ë‹¤. ë˜í•œ training setì™€ì˜ ìœ ì‚¬ì„±ì„ í‰ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤.

ì˜ ë§Œë“  generatorì˜ ì´ë¯¸ì§€ëŠ” ì–´ë–¤ ìŠ¤ì¼€ì¼ì´ë“  ìƒê´€ì—†ì´ localí•œ ë¶€ë¶„ì´ training setê³¼ ìœ ì‚¬í•  ê²ƒì´ë‹¤. Generated images(low resolution  16 x 16)ì™€ target images(low resolution 16 x 16)ë¥¼ Laplacian pyramidì— ë„£ê³ , ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¨ local image patchë“¤ì˜ ë¶„í¬ ì‚¬ì´ì—ì„œ multi-scale statistical similarityë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤.

Training setê³¼ generated setìœ¼ë¡œë¶€í„° ê°ê° local image patchë¥¼ ì–»ëŠ”ë‹¤. ê° color channelì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¡œ patchë“¤ì„ ì •ê·œí™”í•œë‹¤. ì´í›„ sliced Wasserstein distence (SWD)ë¥¼ í†µí•´ statistical similarityë¥¼ ê³„ì‚°í•œë‹¤. `(ì™„ë²½ ì´í•´ x)`

ì‘ì€ Wasserstein distanceëŠ” patchë“¤ ê°„ì˜ ë¶„í¬ê°€ ìœ ì‚¬í•˜ë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚´ê³ , training imagesì™€ generator samplesê°€  spatial resolutionì—ì„œ apperanceì™€ variationê°€ ìœ ì‚¬í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. 

Lowset resolution 16 x 16 imagesë¡œë¶€í„° ì¶”ì¶œëœ patch sets ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” large-scale image structuresì—ì„œ ìœ ì‚¬ì„±ì„ ë‚˜íƒ€ë‚´ê³ , Finest-level patchesëŠ” edgeì™€ noiseì˜ sharpnessì™€ ê°™ì€pixel-level attrubutesì— ëŒ€í•œ ì •ë³´ë¥¼ encodeí•œë‹¤.

<br>

## 6. Experiments

### 6.1. Importance of Individual Contributions in Teams of Statistical Similarity

Contributionì„ í‰ê°€í•˜ê¸° ìœ„í•´ sliced Wasserstein distance (SWD)ì™€ multi-scale structural similarity (MS-SSIM)ì„ ì‚¬ìš©í•œë‹¤. LossëŠ” WGAN-GPë¥¼, ë°ì´í„°ì…‹ì€ CelebAì™€ LUSN bedroomì„ ì‚¬ìš©í•œë‹¤.

<p align="center">
    <img src="..\assets\pggan\table1.PNG" style="zoom:80%;" />
    Table 1
</p>

SWDì™€ MS-SSIMì„ ê³„ì‚°í•œ ê²°ê³¼ì´ë‹¤.

SWDì—ì„œ, ê° ì—´ì€ Laplacian pyramidì˜ levelì„ ë‚˜íƒ€ë‚´ê³ , ë§ˆì§€ë§‰ ì—´ì€ ë„¤ ê°œ distanceì˜ í‰ê· ê°’ì´ë‹¤.

<p align="center">
    <img src="..\assets\pggan\figure3.PNG" />
    Figure 3
</p>
**(a) - (g)** ëŠ” Table 1ì˜ í–‰ì— í•´ë‹¹í•˜ëŠ” CelebA ì˜ˆì œë“¤ì´ë‹¤. ì´ê²ƒë“¤ì€ ë‚´ë¶€ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ì§€ ì•ŠëŠ”ë‹¤. 

**(h)**ëŠ” ìš°ë¦¬ì˜ ìˆ˜ë ´ëœ ê²°ê³¼ì´ë‹¤.

<br>

ì¢‹ì€ í‰ê°€ ì§€í‘œëŠ” ìƒ‰ìƒ, ì§ˆê°, ë°©í–¥ì—ì„œ ë§ì€ variationì´ ìˆëŠ” ê·¸ëŸ´ë“¯í•œ ì´ë¯¸ì§€ì¸ì§€ íŒë‹¨í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.

MS-SSIMì€ outputs ì‚¬ì´ì—ì„œì˜ variationë§Œì„ ì¸¡ì •í•˜ê¸° ë•Œë¬¸ì— generated imagesì™€ training setì˜ ìœ ì‚¬ì„±ì„ íŒë‹¨í•  ìˆ˜ ì—†ë‹¤. ê·¸ëŸ¬ë‚˜, SWDëŠ” generated imagesì˜ ë¶„í¬ê°€ training setì™€ ìœ ì‚¬í•˜ë‹¤ëŠ” ê²ƒì„ ì˜¬ë°”ë¥´ê²Œ ì°¾ëŠ”ë‹¤.

ì²« ë²ˆì§¸ configuration **(a)**ëŠ” Gulrajani et al. (2017)ì´ë‹¤. Generatorì—ì„œ batch normalizationì„ ì‚¬ìš©í•˜ê³  Discriminatorì—ì„œ layer normalizationì„ ì‚¬ìš©í•˜ë©°, minibatch í¬ê¸°ëŠ” 64ì´ë‹¤. 

**(b)**ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ progressive growingì„ ì¶”ê°€í–ˆê³ , ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¸ë‹¤. 

ì£¼ìš”í•œ ëª©í‘œëŠ” ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ë§Œë“œëŠ” ê²ƒì´ë‹¤. ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ë§Œë“œë ¤ë©´ ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ ì¸í•´ mini-batchesì˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì•¼ í•œë‹¤. **(c)**ì—ì„œ minibatch í¬ê¸°ë¥¼ 64ì—ì„œ 16ìœ¼ë¡œ ì¤„ì˜€ë‹¤. ê·¸ ë•Œ ìƒì„±ëœ ì´ë¯¸ì§€ëŠ” ìƒë‹¹íˆ ë¶€ìì—°ìŠ¤ëŸ½ë‹¤. MS-SSIMê³¼ SWDì—ì„œë„ ê²°ê³¼ê°€ ì¢‹ì§€ ì•Šì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

**(d)**ëŠ” hyperparametersë¥¼ ì¡°ì •í•´ì„œ training processë¥¼ ì•ˆì •í™”ì‹œí‚¤ê³ , batch normalizationê³¼ layer normalizationì„ ì œê±°í–ˆë‹¤.

**(e*)**ëŠ” minibatch discriminationì„ ì‚¬ìš©í•˜ì§€ë§Œ, ê·¸ë‹¤ì§€ í‰ê°€ ì§€í‘œì˜ ê°’ë“¤ì„ ê°œì„ í•˜ì§€ ëª»í•œë‹¤.

ëŒ€ì¡°ì ìœ¼ë¡œ ìš°ë¦¬ì˜ minibatch standard deviation **(e)**ëŠ” average SWDì™€ ì´ë¯¸ì§€ë“¤ì„ ê°œì„ í•œë‹¤.

ê·¸ëŸ° í›„, **(f)**ì™€ **(g)**ì—ì„œ ìš°ë¦¬ì˜ contributionì„ ì¶”ê°€í•´ì„œ, ì „ë°˜ì ì¸ SWDë¥¼ í–¥ìƒì‹œí‚¤ê³  ì´ë¯¸ì§€ì˜ ì§ˆì„ ë†’ì¸ë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ, **(h)**ì—ì„œ ì œëŒ€ë¡œ ëœ ë„¤íŠ¸ì›Œí¬(PGGAN)ë¥¼ ì‚¬ìš©í•˜ê³  ë” ì˜¤ë«ë™ì•ˆ training í•œë‹¤ - ê²°ê³¼ê°€ ê°€ì¥ ì¢‹ë‹¤.

<br>

### 6.2. Convergence and Training Speed

<p align="center">
    <img src="..\assets\pggan\figure4.PNG" style="zoom: 80%;" />
    Figure 4
</p>

Figure 4ëŠ” SWD metricê³¼ raw image throughputì˜ ê´€ì ì—ì„œ progressive growingì˜ íš¨ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. 

NVIDIA Tesla P100ì„ ì‚¬ìš©í•œ single-GPU setupì—ì„œ ì¸¡ì •í–ˆë‹¤.

**(a)** 128 x 128 í•´ìƒë„ë¥¼ ê°€ì§„ CelebAë¥¼ Gulrajani et al. (2017) ë„¤íŠ¸ì›Œí¬ì— ë„£ê³  trainingì„ í•œ ê²ƒì´ë‹¤. ê° ê·¸ë˜í”„ëŠ” Laplacian pyramidì˜ í•œ ë ˆë²¨ì—ì„œ sliced Wasserstein distanceë¥¼ ë‚˜íƒ€ë‚´ê³ , ìˆ˜ì§ì„ ì€ Table 1ì—ì„œ trainingì„ ì¤‘ë‹¨í•œ ì§€ì ì´ë‹¤.

**(b)** Gulrajani et al. (2017)ì— progressive growingì„ ì¶”ê°€í•œ ê·¸ë˜í”„ì´ë‹¤. ì ì„ ìœ¼ë¡œ ëœ ìˆ˜ì§ì„ ì€ Gì™€ Dì˜ í•´ìƒë„ë¥¼ ë‘ ë°°ë¡œ ë§Œë“¤ ì§€ì ì„ ë‚˜íƒ€ë‚¸ë‹¤.

**(C)** 1024 x 1024 í•´ìƒë„ì—ì„œ raw training speedì—ì„œ progressive growingì˜ íš¨ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

<br>

**(b)**ëŠ” better optimumì— ìƒë‹¹íˆ ì˜ ìˆ˜ë ´í•˜ê³ , ì´ í›ˆë ¨ì‹œê°„ì„ ì•½ 2ë°° ë‹¨ì¶•í•œë‹¤. 

Progressive growingì´ ì—†ìœ¼ë©´, generatorì™€ discriminatorì˜ ëª¨ë“  ë ˆì´ì–´ë“¤ì€ large-scale variationê³¼ small-scale detailì„ ìœ„í•´ ê°„ê²°í•œ intermediate representationsë¥¼ ë™ì‹œì— ì°¾ëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜ progressive growingì´ ìˆìœ¼ë©´, ê¸°ì¡´ì˜ low-resolution layersëŠ” ì´ë¯¸ ì¼ì° ìˆ˜ë ´ë  ê°€ëŠ¥ì„±ì´ ìˆì–´ì„œ ë„¤íŠ¸ì›Œí¬ëŠ” ìƒˆë¡œìš´ ë ˆì´ì–´ê°€ ë„ì…ë¨ì— ë”°ë¼ ì ì  ë” ì‘ì€ ìŠ¤ì¼€ì¼ì˜ íš¨ê³¼ë¡œ representationsë¥¼ êµ¬ì²´í™”í•˜ëŠ” ì‘ì—…ë§Œ ë‹´ë‹¹í•œë‹¤.

ì‹¤ì œë¡œ, Figure 4**(b)**ì—ì„œ largest-scale statistical similarity curve (16)ì´ optimal valueì— ë§¤ìš° ë¹ ë¥´ê²Œ ë„ë‹¬í•œë‹¤. Smaller-scale curves (32, 64, 128)ëŠ” ë™ë“±í•˜ê²Œ ì¼ê´€ì„± ìˆê²Œ ìˆ˜ë ´í•œë‹¤. 

Non-progressive trainingì¸ Figure 4**(a)**ì—ì„œ SWD metricì˜ ê° scaleì€ ì˜ˆìƒëŒ€ë¡œ ì¼ì •í•˜ê²Œ ìˆ˜ë ´ëœë‹¤.

Figure 4**(c)**ì—ì„œ progressive growingì€ í•´ìƒë„ê°€ ì¦ê°€í•˜ë©´ ì†ë„ê°€ ì¦ê°€í•œë‹¤. 

<br>

### 6.3. High-Resolution Image Generator using Celeba-HQ Dataset

High output resolutionsì—ì„œ ê²°ê³¼ë¥¼ ì˜ë¯¸ìˆê²Œ ì„¤ëª…í•˜ê¸° ìœ„í•´ì„œ, ì¶©ë¶„íˆ ë‹¤ì–‘í•œ ê³ í’ˆì§ˆ ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤. ê·¸ëŸ¬ë‚˜, ì´ì „ì— GAN ë¬¸í—Œì—ì„œ ì‚¬ìš©ëœ ê±°ì˜ ëª¨ë“  ê³µê°œ ë°ì´í„° ì„¸íŠ¸ëŠ” $32^2$ì—ì„œ $480^2$ê¹Œì§€ ìƒëŒ€ì ìœ¼ë¡œ low resolutionsì´ë‹¤. ì´ë¥¼ ìœ„í•´, 1024 x 1024 í•´ìƒë„ì˜ 30000ê°œì˜ ì´ë¯¸ì§€ë“¤ë¡œ êµ¬ì„±ëœ CelebAì˜ ê³ í’ˆì§ˆ ë²„ì „ì„ ë§Œë“¤ì—ˆë‹¤. 

<br>

<p align="center">
    <img src="..\assets\pggan\figure5.PNG" style="zoom:80%;" />
    Figure 5
</p>

CelebA-HQ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ì„œ ë§Œë“  1024 x 1024 ì´ë¯¸ì§€ì´ë‹¤.

<br>

8ê°œì˜ Tesla V100 GPUsë¥¼ ê°€ì§€ê³  4ì¼ë™ì•ˆ trainí–ˆë‹¤. ìš°ë¦¬ì˜ implementationì€ í•´ë‹¹ output resolutionì— ë”°ë¥¸ adaptive minibatch sizeë¥¼ ì‚¬ìš©í•´ì„œ ìœ íš¨í•œ memory budgetê°€ ìµœì ìœ¼ë¡œ ì‚¬ìš©ë˜ì—ˆë‹¤.

ìš°ë¦¬ì˜ contributionsì´ loss functionì— ë…ë¦½ì ì´ë¼ëŠ” ê²ƒì„ ì„¤ëª…í•˜ê¸° ìœ„í•´, WGAN-GP ëŒ€ì‹  LSGAN lossë¥¼ ì‚¬ìš©í•´ì„œ ê°™ì€ ë„¤íŠ¸ì›Œí¬ë¥¼ trainí–ˆë‹¤. 

Figure 1ì€ LSGANì„ ì‚¬ìš©í•˜ì—¬ ë§Œë“  ì´ë¯¸ì§€ì´ë‹¤. 

<br>

### 6.4. LUSN Results

<p align="center">
    <img src="..\assets\pggan\figure6.PNG" style="zoom:90%;" />
    Figure 6
</p>

<p align="center">
    <img src="..\assets\pggan\figure7.PNG" style="zoom:80%;" />
    Figure 7
</p>

Figure 6ì€ LSUN BEDROOMì—ì„œ ê²°ê³¼ë¥¼ ë¹„êµí•œ ê²ƒì´ë‹¤.

Figure 7ì€ $256^2$í•´ìƒë„ì˜ ë‹¤ì–‘í•œ LSUN categoriesì—ì„œì˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. 

<br>

### 6.5. CIFAR10 Inception Scores

ìš°ë¦¬ì˜ ë°©ë²•ì„ ì‚¬ìš©í•´ì„œ 8.80ì´ë¼ëŠ” ë†’ì€ ì ìˆ˜ë¥¼ ì–»ì—ˆë‹¤.

<br>

<br>

ğŸ˜„ğŸ˜„PGGANì˜ ë” ì„¸ë¶€ì ì¸ ì •ë³´ë¥¼ ì•Œê³  ì‹¶ìœ¼ë©´ ë…¼ë¬¸ì˜ ë¶€ë¡ì„ ì°¸ê³ í•˜ë©´ ëœë‹¤.ğŸ˜„ğŸ˜„

------

Reference

[https://towardsdatascience.com/progan-how-nvidia-generated-images-of-unprecedented-quality-51c98ec2cbd2](https://towardsdatascience.com/progan-how-nvidia-generated-images-of-unprecedented-quality-51c98ec2cbd2)

[https://wiserloner.tistory.com/1196](https://wiserloner.tistory.com/1196)

[https://sensibilityit.tistory.com/508?category=731657](https://sensibilityit.tistory.com/508?category=731657)

[https://aigong.tistory.com/65](https://aigong.tistory.com/65)